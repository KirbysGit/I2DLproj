# Retail Product Detection Model

A PyTorch-based object detection framework specifically designed for detecting retail products in densely packed shelf images. This project implements a custom object detection model with features tailored for retail environments.

## Project Overview

This project focuses on developing an object detection system for the SKU-110K dataset, which contains challenging retail shelf images with densely packed products. Our model is designed to handle:
- Dense object detection scenarios
- Multiple objects with similar appearances
- Varying scales of products
- Overlapping items

### Current Development Status

âœ… Completed:
- Basic model architecture implementation
- Anchor generation and matching system
- Loss functions (classification and box regression)
- Training pipeline with validation
- Initial data loading and augmentation

ğŸ”„ In Progress:
- Improving model performance on dense object scenarios
- Enhancing visualization tools
- Documentation and code organization

## Project Structure
```
i2dlproj/
â”œâ”€â”€ v1/                       # Legacy version (initial baseline implementation)
â”‚   â”œâ”€â”€ ...                   # Old source code and training pipeline
â”‚
â”œâ”€â”€ restart/                 # Current implementation (modular, improved architecture)
â”‚   â”œâ”€â”€ model/               # Core model components: backbone, FPN, detection head, anchors
â”‚   â”‚   â”œâ”€â”€ anchor_generator.py
â”‚   â”‚   â”œâ”€â”€ backbone.py
â”‚   â”‚   â”œâ”€â”€ detection_head.py
â”‚   â”‚   â”œâ”€â”€ detector.py
â”‚   â”‚   â””â”€â”€ fpn.py
â”‚   â”‚ 
â”‚   â”œâ”€â”€ data/                # Dataset loader and retail image preprocessing
â”‚   â”‚   â””â”€â”€ dataset.py
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/               # Helper functions for box operations and plotting
â”‚   â”‚   â”œâ”€â”€ box_ops.py
â”‚   â”‚   â”œâ”€â”€ plots.py
â”‚   â”‚   â””â”€â”€ visualize_detections.py
â”‚   â”‚
â”‚   â”œâ”€â”€ train/               # Training loop and training logic
â”‚   â”‚   â””â”€â”€ trainer.py
â”‚   â”‚
â”‚   â”œâ”€â”€ test/                # Unit tests for model components
â”‚   â”‚   â”œâ”€â”€ test_anchor_coverage.py
â”‚   â”‚   â”œâ”€â”€ test_anchor_generator.py
â”‚   â”‚   â”œâ”€â”€ test_anchor_matching.py
â”‚   â”‚   â”œâ”€â”€ test_box_iou.py
â”‚   â”‚   â”œâ”€â”€ test_dataset.py
â”‚   â”‚   â”œâ”€â”€ test_detection_head.py
â”‚   â”‚   â”œâ”€â”€ test_detector.py
â”‚   â”‚   â”œâ”€â”€ test_overfitting.py
â”‚   â”‚   â””â”€â”€ test_pipeline.py
â”‚   â”‚
â”‚   â”œâ”€â”€ config/              # YAML configuration files for training/evaluation
â”‚   â”‚   â”œâ”€â”€ testing_config.yaml
â”‚   â”‚   â””â”€â”€ training_config.yaml
â”‚   â”‚
â”‚   â”œâ”€â”€ test.py              # Evaluation entry point for running tests
â”‚   â””â”€â”€ compareSOTA.py       # Evaluation script for benchmarking against SOTA (YOLOv5)
â”‚
â”œâ”€â”€ training_runs/           # Saved training checkpoints, logs, and visualizations
â”‚   â””â”€â”€ TR_{timestamp}/
â”‚       â”œâ”€â”€ checkpoints/
â”‚       â”œâ”€â”€ visualizations/
â”‚       â””â”€â”€ training_loss.png
â”‚
â”œâ”€â”€ test_runs/               # Evaluation outputs for specific model runs
â”‚   â””â”€â”€ eval_{model}_{timestamp}/
â”‚       â”œâ”€â”€ metrics/
â”‚       â”œâ”€â”€ visualizations/
â”‚       â””â”€â”€ eval_config.yaml
â”‚
â”œâ”€â”€ comparison_results/      # YOLOv5 vs ShelfVision comparison outputs
â”œâ”€â”€ checkpoints/             # Manually saved model weights
â”œâ”€â”€ debug_output/            # Intermediate debug images and logs
â”œâ”€â”€ test_results/            # Unit test output and logs
â”œâ”€â”€ docs/                    # Project documentation, slides, and notes
â””â”€â”€ requirements.txt         # Python dependencies list


```

The project follows a modular structure where:
- `restart/`: Contains all source code and modular implementation for the latest model version
  - `model/`: Backbone, FPN, anchors, and detection logic
  - `data/`: Dataset loading and preprocessing
  - `utils/`: Visualization and box operations
  - `train/`: Core training loop logic
  - `test/`: Unit tests for model components
  - `config/`: Config files for training and testing
  - `compareSOTA.py` and `test.py`: Entry points for evaluation
- `test_runs/`: Stores model evaluation results, metrics, and visualizations
- `comparison_results/`: Output comparisons against YOLOv5 baseline models
- `training_runs/`: Contains training logs and outputs
- `debug_output/`: Debug visualizations and outputs
- `test_results/`: Test execution outputs
- `docs/`: Project documentation
- `checkpoints/`: Saved model weights and states

âš ï¸ **Legacy Note**: The `v1/` folder contains the original baseline version used in early development (Eval 1). 
The `restart/` directory includes the updated model logic, modular pipeline, and improvements featured in Eval 2.

## Key Features

### Model Architecture
- Feature Pyramid Network (FPN) backbone
- Multi-scale detection heads
- Anchor-based detection system
- Binary classification (object vs. background)

### Training Features
- Batch-based training with validation
- IoU-based anchor matching
- Smooth L1 loss for box regression
- Binary Cross Entropy for classification
- Learning rate scheduling

## Installation

1. Clone the repository:
```bash
git clone <repository_url>
cd retail-detection
```

2. Create and activate a virtual environment:

For Windows:
```bash
# Create virtual environment
python -m venv venv

# Activate virtual environment
.\venv\Scripts\activate
```

For Linux/Mac:
```bash
# Create virtual environment
python -m venv venv

# Activate virtual environment
source venv/bin/activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Download the SKU-110K dataset and place it in the `datasets` directory.

## Usage

The project uses YAML configuration files to manage training and testing parameters. The main workflow involves:

1. Adjusting the configuration files in `restart/config/`:
   - `training_config.yaml`: Training parameters and model settings
   - `testing_config.yaml`: Evaluation and testing parameters

2. Running the training or testing scripts:

### Training
```bash
# Edit restart/config/training_config.yaml first to set your parameters
python restart/train/trainer.py
```

### Testing/Evaluation
```bash
# Edit restart/config/testing_config.yaml first to set your parameters
python restart/test.py
```

## Configuration Files

### Training Configuration
Key parameters in `restart/config/training_config.yaml`:
```yaml
# Model Parameters
model:
  backbone: "resnet50"
  num_classes: 1
  num_anchors: 9
  pretrained: true

# Training Parameters
training:
  batch_size: 16
  num_epochs: 20
  learning_rate: 0.001
  save_freq: 5
  num_workers: 4

# Dataset Parameters
dataset:
  image_size: [640, 640]
  train_split: 0.8
  augmentation: true

# Output Settings
output_dir: "training_runs"
checkpoint_dir: "checkpoints"
```

### Testing Configuration
Key parameters in `restart/config/testing_config.yaml`:
```yaml
# Model Parameters
checkpoint_path: "checkpoints/best_model.pth"
confidence_threshold: 0.5
nms_threshold: 0.3

# Evaluation Parameters
num_images: 100
batch_size: 16
visualize: true

# Output Settings
output_dir: "test_runs"
```

### Example Workflows

1. **Training Pipeline**
```bash
# 1. Edit training configuration
vim restart/config/training_config.yaml

# 2. Run training
python restart/train/trainer.py

# 3. Edit testing configuration
vim restart/config/testing_config.yaml

# 4. Run evaluation
python restart/test.py
```

2. **Evaluation Only**
```bash
# 1. Edit testing configuration to point to your model checkpoint
vim restart/config/testing_config.yaml

# 2. Run evaluation
python restart/test.py
```

### Monitoring and Visualization

- Training metrics are saved in `training_runs/<run_name>/`
- Visualizations are saved in `debug_output/` when debug mode is enabled in config
- Evaluation results are saved in `test_runs/` directory
- Use TensorBoard for real-time monitoring:
```bash
tensorboard --logdir training_runs/
```